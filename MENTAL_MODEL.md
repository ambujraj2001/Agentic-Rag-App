# RAG Assistant — Mental Model

This document explains how the RAG Assistant works at a conceptual level. No code — just the thinking behind every layer, decision, and data flow.

---

## The Core Idea

RAG stands for **Retrieval Augmented Generation**. The fundamental problem it solves is this: Large Language Models are powerful at generating text, but they only know what they were trained on. If you want an LLM to answer questions about *your* documents — internal wikis, personal notes, company data — it has no idea what's in them.

RAG bridges that gap with a two-phase approach:

1. **Retrieval** — Find the most relevant pieces of your documents that relate to the user's question.
2. **Generation** — Hand those pieces to an LLM along with the question and let it synthesize a natural-language answer grounded in your actual data.

This project implements both phases as a full-stack web application.

---

## The Two Brains

The system has two distinct "intelligence" components, and understanding their separation is key to the entire architecture.

### Brain 1: The Embedding Model (Local)

This brain lives entirely on your machine, inside a Docker container. Its job is to convert text into **vectors** — arrays of numbers that capture the *meaning* of a piece of text. Two sentences that mean similar things will produce vectors that are close together in mathematical space, even if they use completely different words.

The specific model used is `sentence-transformers/multi-qa-MiniLM-L6-cos-v1` — a small, fast model optimized for question-answering scenarios. It runs on CPU (no GPU required) and handles all embedding work without any external API calls. This means your document content never leaves your machine during ingestion or retrieval.

### Brain 2: The Language Model (Remote)

This brain lives on HuggingFace's servers. It's Qwen 2.5 7B — a general-purpose language model accessed through HuggingFace's Router API. Its job is the "generation" half: given some context chunks and a question, it writes a coherent, human-readable answer.

Only this second brain makes external API calls. And crucially, it only ever sees the *relevant chunks* retrieved for a specific question — not your entire document corpus.

### Why This Split Matters

Embedding is done constantly — every document ingested and every question asked requires an embedding operation. Keeping this local means zero latency to external services, zero cost per embedding, and full privacy of your raw documents. Generation happens less frequently (once per question) and benefits from the much larger, more capable remote model.

---

## The Data Store: Weaviate

Weaviate is a **vector database** — a database purpose-built for storing and searching vectors. Traditional databases find rows by matching column values. Weaviate finds objects by finding vectors that are *close* to a query vector.

Under the hood, Weaviate uses an HNSW (Hierarchical Navigable Small World) index — a data structure that makes approximate nearest-neighbor search extremely fast, even with millions of vectors.

### What Gets Stored

When you ingest a document, Weaviate stores three things together as a single object:

- **The text content** itself (the raw document text)
- **The vector** (automatically generated by the transformer container — you never handle this yourself)
- **Metadata** — a source label and a timestamp

Weaviate's `text2vec-transformers` module handles the vector generation transparently. When an object is inserted, Weaviate automatically sends the text content to the transformer container, gets back a vector, and stores both together. This means the backend doesn't need to know anything about vectors or embeddings — it just stores text and lets Weaviate handle the rest.

### How Search Works

When a question comes in, Weaviate does the same thing in reverse: it sends the question text to the transformer container to get a vector, then finds the 5 stored objects whose vectors are closest to the question vector. "Closest" here means cosine similarity — a measure of how much two vectors point in the same direction, regardless of their magnitude.

The result is a ranked list of the most semantically relevant document chunks, each with a distance score converted to a similarity score (1.0 = perfect match, 0.0 = completely unrelated).

---

## The Docker Architecture

Two containers run side by side on your machine, connected by a private Docker network:

### Container 1: Weaviate

The vector database. It exposes port 8090 to your host machine so the backend application can talk to it. It handles all storage, indexing, and search operations.

### Container 2: Transformer Inference

The embedding model server. It does *not* expose any ports to your host machine — it only communicates with Weaviate over the internal Docker network. Weaviate sends it text, it returns vectors. Nothing else talks to it directly.

This is a deliberate architectural choice. The transformer service is an implementation detail of the vector database, not something the application needs to know about. From the backend's perspective, Weaviate is a single service that accepts text and returns semantically similar text.

### Data Persistence

Weaviate's data is stored in a Docker volume (`weaviate_data`). This means your ingested documents survive container restarts — you don't need to re-ingest every time you restart the application. However, `docker compose down -v` would destroy this volume and all stored data.

The transformer container is stateless. The model weights are baked into its Docker image (~400MB). It has no data to persist.

---

## The Backend: Orchestrator

The backend is a Node.js/Express/TypeScript application. Its job is simple: it sits between the frontend and the two brains, orchestrating the flow.

### Request Flow for Ingestion

When a user submits a document:

1. The controller validates the input (is there actually text to ingest?)
2. The Weaviate service inserts the text, along with a source label and timestamp
3. Weaviate automatically generates and stores the vector via the transformer container
4. A document ID is returned to confirm success

The backend has no knowledge of vectors or embeddings here. It's just passing text to Weaviate.

### Request Flow for a Question

When a user asks a question, a more interesting chain unfolds:

1. The controller validates the input
2. The Weaviate service performs a **semantic search** — "find the 5 chunks most similar to this question"
3. If nothing relevant is found, a fallback message is returned immediately (no LLM call wasted)
4. If chunks are found, the LLM service constructs a **prompt** — a carefully structured message that contains:
   - A system instruction telling the model to answer based on provided context and to admit when information is insufficient
   - The retrieved chunks, numbered for reference
   - The user's question
5. This prompt is sent to HuggingFace's chat completion API
6. The LLM's response is packaged with the source chunks (including their relevance scores) and sent back to the frontend

### Why the Backend Serves the Frontend

In production, the Express server serves the built frontend as static files from the same port (3001). This is a single-deployment architecture — you don't need a separate web server or CDN. One URL, one port, everything works.

During development, the frontend runs its own Vite dev server with hot reload, and proxies API requests to the backend. This gives the best of both worlds: fast frontend iteration with live backend integration.

### Error Philosophy

The backend uses a layered error approach:

- **Validation errors** (400): Bad input from the user — empty question, missing content
- **Service errors** (502): External dependencies failed — HuggingFace API is down, Weaviate is unreachable
- **Unknown errors** (500): Unexpected failures caught by the centralized error handler

Every error is wrapped in a consistent format with a message and status code. The frontend never sees a raw stack trace in production. There's also structured JSON logging with timestamps and context tags, so when something fails, you can trace exactly where and when.

---

## The Frontend: Conversational Interface

The frontend is a React + TypeScript application with two panels, built with Vite.

### Ingest Panel

A simple text area where users paste or type document content and submit it. It's intentionally minimal — just input, a submit button, and feedback (success or error). The mental model for the user is: "I'm teaching the assistant about something."

### Chat Panel

A conversational interface that looks and feels like a messaging app. Messages appear in a scrolling list — user messages on the right, assistant messages on the left. This familiar pattern reduces cognitive load; users already know how to interact with a chat.

Each assistant message can optionally show **sources** — the document chunks that informed the answer, with relevance scores. This is expandable (collapsed by default) to avoid clutter while still giving transparency. Users can verify *why* the assistant said what it said, which builds trust.

### State Management

The frontend uses React's built-in state (no Redux, no Zustand). Each panel has its own custom hook that encapsulates all state logic and API interactions:

- `useChat` manages the message list, loading state, and error handling for questions
- `useIngest` manages the submission state for document ingestion

This separation means the UI components only care about rendering. All "what happens when the user clicks send" logic lives in the hooks. If you wanted to replace the chat UI entirely, the hook would still work unchanged.

### API Communication

A thin API client handles all HTTP communication with the backend. In production, it uses relative URLs (`/api/...`) which naturally resolve to the same origin since the backend serves the frontend. In development, Vite's proxy configuration redirects these same URLs to `localhost:3001`.

---

## The Startup Sequence

The `start.sh` script orchestrates a careful startup sequence, and each step exists for a reason:

1. **Prerequisites check** — Fails fast if Docker, Node, or npm are missing rather than failing cryptically later
2. **Environment setup** — Creates `.env` from the example if it doesn't exist, so first-time users don't get a confusing "missing env" error
3. **Docker services** — Starts Weaviate and the transformer container. Weaviate depends on the transformer service, so Docker starts the transformer first
4. **Health check loop** — Polls Weaviate's readiness endpoint every 3 seconds for up to 3 minutes. The transformer model needs time to load into memory (~30–60 seconds on first start), and Weaviate needs the transformer to be ready before it reports healthy
5. **Dependency installation** — Only runs `npm install` if `node_modules` doesn't exist, skipping this on subsequent starts
6. **Build** — Compiles both backend TypeScript and frontend React/Vite
7. **Server start** — Launches the backend in the background, saves its PID for clean shutdown
8. **Cleanup trap** — Registers signal handlers so Ctrl+C cleanly stops the backend process

The shutdown script reverses this: kills the backend process (gracefully, then forcefully if needed), then stops Docker services.

---

## The Prompt Engineering

The way the system talks to the LLM is carefully structured:

**System message**: Tells the model its role — answer based on provided context, and explicitly say so if the context doesn't contain enough information. This prevents hallucination; the model is instructed not to make things up.

**User message**: Contains the retrieved chunks as numbered sections, followed by the question. Numbering the chunks gives the model a way to mentally "reference" specific pieces of context, leading to more grounded answers.

**Temperature setting (0.7)**: A moderate temperature that allows some creativity in phrasing while staying mostly faithful to the context. Lower would make answers more deterministic but potentially stilted; higher would risk more hallucination.

---

## Key Architectural Decisions and Why

### Local Embeddings, Remote Generation

Embeddings are a commodity operation — the quality difference between models is small, and the operation is frequent. Running it locally eliminates a dependency, reduces latency, and preserves privacy. Generation is rare (once per query) and benefits enormously from a large, capable model that would be impractical to run locally.

### Single Collection, No Chunking

Documents are stored as-is in a single Weaviate collection. There's no built-in text splitting or chunking strategy. This keeps the system simple but means very long documents should ideally be split by the user before ingestion. The embedding model has a token limit, and excessively long text will be truncated or poorly represented as a single vector.

### Monolithic Deployment

One port serves everything — API and frontend. This eliminates CORS issues, simplifies deployment, and means there's one thing to start and one thing to stop. The trade-off is you can't scale the frontend and backend independently, but for a single-user or small-team tool, this is the right simplicity trade-off.

### No Authentication

The system trusts all requests. Weaviate runs with anonymous access enabled. There are no API keys on the backend. This is appropriate for local/development use but would need an auth layer for any shared deployment.

### TypeScript Everywhere

Both frontend and backend use TypeScript with strict settings. Shared type definitions (request/response shapes) ensure the API contract is explicit. If someone changes the API response format, TypeScript catches mismatches at compile time rather than at runtime.

---

## What This System Is (and Isn't)

**It is**: A complete, working RAG pipeline that lets you ingest your own documents and ask questions about them using a modern LLM, with local embeddings and a clean web interface.

**It is not**: A production system for large-scale deployment. It lacks authentication, horizontal scaling, document chunking strategies, conversation memory, streaming responses, and rate limiting. These are all things that could be added, but the current architecture is deliberately simple to serve as a clear, understandable foundation.

The entire system can be understood as a pipeline: **Text → Vectors → Storage → Retrieval → Prompt → LLM → Answer**. Every component serves exactly one step in that pipeline.
